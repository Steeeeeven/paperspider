# UK Biobank 爬虫 - 摘要重试脚本使用说明

## 问题描述

在爬取过程中，发现许多文章的摘要显示为"未找到摘要"，需要重新获取这些文章的摘要信息。

## 解决方案

提供了两个摘要重试脚本来重新获取缺失的摘要：

### 1. 完整摘要重试脚本 (`retry_abstracts.py`)

**功能特点：**
- ✅ 自动识别缺少摘要的文章
- ✅ 并发重新获取摘要（默认5个线程）
- ✅ 自动合并到原始数据文件
- ✅ 详细的进度显示和统计
- ✅ 支持多种输出格式

**使用方法：**

```bash
python retry_abstracts.py
```

**输出文件：**
- `abstract_retry_YYYYMMDD_HHMMSS.csv` - 重新获取摘要的文章
- `abstract_retry_YYYYMMDD_HHMMSS.json` - JSON格式
- `merged_with_abstract_YYYYMMDD_HHMMSS.csv` - 合并后的完整数据

### 2. 快速摘要重试脚本 (`quick_retry_abstracts.py`)

**功能特点：**
- ✅ 简单快速
- ✅ 顺序处理（更稳定）
- ✅ 适合快速测试

**使用方法：**

```bash
python quick_retry_abstracts.py
```

**输出文件：**
- `abstract_retry_YYYYMMDD_HHMMSS.csv` - 重新获取摘要的文章
- `abstract_retry_YYYYMMDD_HHMMSS.json` - JSON格式

## 推荐使用流程

### 步骤1：运行摘要重试脚本

```bash
# 推荐使用完整脚本（自动合并）
python retry_abstracts.py

# 或使用快速脚本
python quick_retry_abstracts.py
```

### 步骤2：查看结果

脚本会显示：
- 找到多少篇缺少摘要的文章
- 成功重新获取摘要的文章数
- 失败的文章数
- 成功率统计

### 步骤3：手动合并（如果需要）

如果使用快速脚本，可能需要手动合并结果到原始文件中。

## 预期结果

### 可能的情况

1. **部分成功** - 一些文章确实有摘要，之前可能是网络问题
2. **全部失败** - 这些文章可能确实没有摘要
3. **大部分成功** - 网络问题导致的摘要获取失败

### 典型输出示例

```
使用文件: publications_heart_concurrent.csv
找到 156 篇缺少摘要的文章
================================================================================
开始重新获取摘要...
================================================================================

[1/156] 重新获取摘要...
标题: Association between cardiovascular risk factors and...
✓ 成功

[2/156] 重新获取摘要...
标题: Genetic variants associated with heart disease...
✓ 成功

...

================================================================================
摘要重试完成！
================================================================================
总文章数: 156
成功获取: 142
获取失败: 14
成功率: 91.0%
耗时: 480.25 秒

输出文件:
  - abstract_retry_20241201_150322.csv
  - abstract_retry_20241201_150322.json

摘要统计:
  - 有摘要: 142 篇
  - 无摘要: 14 篇
```

## 摘要获取失败的可能原因

### 1. 文章确实没有摘要
- 某些文章可能确实没有提供摘要
- 这是正常情况

### 2. 页面结构问题
- 摘要可能在不同的HTML结构中
- 需要更新解析逻辑

### 3. 网络问题
- 页面加载不完整
- 网络超时

### 4. 反爬虫机制
- 网站检测到频繁访问
- 需要调整访问频率

## 故障排除

### 1. 如果成功率很低（<50%）

**可能原因：**
- 网络连接不稳定
- 网站结构发生变化
- 反爬虫机制

**解决方案：**
- 检查网络连接
- 降低并发数
- 增加延迟时间

### 2. 如果所有文章都失败

**可能原因：**
- Chrome浏览器问题
- 网站完全无法访问
- 代码逻辑错误

**解决方案：**
- 检查Chrome浏览器
- 手动访问几个文章链接
- 检查错误日志

### 3. 如果脚本运行出错

**检查：**
- Chrome浏览器是否安装
- 网络连接是否正常
- 依赖包是否完整安装

## 手动合并数据

如果使用快速脚本，可以手动合并结果：

```python
import csv
import json

# 读取原始数据
original_data = []
with open('publications_heart_concurrent.csv', 'r', encoding='utf-8-sig') as f:
    reader = csv.DictReader(f)
    original_data = list(reader)

# 读取新摘要数据
abstract_data = []
with open('abstract_retry_20241201_150322.csv', 'r', encoding='utf-8-sig') as f:
    reader = csv.DictReader(f)
    abstract_data = list(reader)

# 创建摘要映射
abstract_map = {item['link']: item['abstract'] for item in abstract_data}

# 更新原始数据
for item in original_data:
    if item['link'] in abstract_map:
        new_abstract = abstract_map[item['link']]
        if new_abstract not in ['未找到摘要', '获取失败', '']:
            item['abstract'] = new_abstract

# 保存合并后的数据
with open('merged_final.csv', 'w', encoding='utf-8-sig', newline='') as f:
    fieldnames = ['title', 'date', 'authors', 'journal', 'publish_date', 'pubmed_id', 'doi', 'link', 'abstract']
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(original_data)
```

## 注意事项

1. **网络友好**：脚本保留了合理的延迟，不会给服务器造成压力
2. **数据安全**：所有数据都会实时保存，不会丢失
3. **资源管理**：自动管理浏览器资源，确保正确释放
4. **异常处理**：完善的错误处理机制

## 文件说明

| 文件名 | 用途 | 说明 |
|--------|------|------|
| `retry_abstracts.py` | 完整摘要重试脚本 | 功能全面，自动合并 |
| `quick_retry_abstracts.py` | 快速摘要重试脚本 | 简单快速，适合测试 |
| `abstract_retry_*.csv` | 摘要重试结果 | 包含重新获取摘要的文章 |
| `abstract_retry_*.json` | 摘要重试结果 | JSON格式的摘要重试结果 |

## 下一步

摘要重试完成后：

1. **检查结果**：查看重新获取的摘要数量和质量
2. **数据合并**：将新摘要合并到主数据文件
3. **质量检查**：检查摘要的完整性和准确性
4. **最终统计**：更新总体的数据统计信息

---

**提示**：建议先运行 `quick_retry_abstracts.py` 进行快速测试，确认功能正常后再使用完整的摘要重试脚本。
