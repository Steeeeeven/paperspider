# UK Biobank 爬虫 - 补偿脚本使用说明

## 问题描述

在爬取过程中，以下页面未找到文章列表：
**5, 7, 14, 27, 38, 40, 41, 46, 47, 92, 111, 135, 146, 154, 181, 200, 209, 220, 223**

## 解决方案

提供了两个补偿脚本来重新爬取这些页面：

### 1. 完整补偿脚本 (`compensate_missing_pages.py`)

**功能特点：**
- ✅ 支持多种模式（普通/重试/合并）
- ✅ 详细的进度显示和统计
- ✅ 自动生成时间戳文件名
- ✅ 支持数据合并功能

**使用方法：**

```bash
# 普通补偿模式
python compensate_missing_pages.py

# 带重试的补偿模式（推荐）
python compensate_missing_pages.py retry

# 数据合并模式
python compensate_missing_pages.py merge
```

**输出文件：**
- `compensate_missing_pages_YYYYMMDD_HHMMSS.csv`
- `compensate_missing_pages_YYYYMMDD_HHMMSS.json`

### 2. 快速补偿脚本 (`quick_compensate.py`)

**功能特点：**
- ✅ 简单快速
- ✅ 固定文件名
- ✅ 适合快速测试

**使用方法：**

```bash
python quick_compensate.py
```

**输出文件：**
- `compensate_missing.csv`
- `compensate_missing.json`

## 推荐使用流程

### 步骤1：运行补偿脚本

```bash
# 推荐使用带重试的完整补偿脚本
python compensate_missing_pages.py retry
```

### 步骤2：查看结果

脚本会显示：
- 成功爬取的页面数
- 失败的页面数
- 总文章数
- 耗时统计

### 步骤3：数据合并（可选）

如果补偿成功获取了新数据，可以合并到主数据文件中：

```bash
python compensate_missing_pages.py merge
```

## 预期结果

### 可能的情况

1. **部分页面成功** - 这些页面确实有文章，之前可能是网络问题
2. **全部失败** - 这些页面可能确实为空页面
3. **部分成功** - 混合情况

### 典型输出示例

```
================================================================================
UK Biobank 爬虫 - 补偿脚本（带重试）
================================================================================
需要补偿的页面: [5, 7, 14, 27, 38, 40, 41, 46, 47, 92, 111, 135, 146, 154, 181, 200, 209, 220, 223]
总页数: 19
================================================================================

[1/19] 正在补偿第 5 页 (尝试 1/3)...
✓ 第 5 页补偿成功 | 文章数: 10

[2/19] 正在补偿第 7 页 (尝试 1/3)...
✗ 第 7 页补偿失败: 未找到文章列表

...

================================================================================
补偿爬取完成！
================================================================================
补偿页数: 19
成功页数: 12
失败页数: 7
总文章数: 120
耗时: 180.45 秒
平均速度: 0.67 篇/秒

成功的页面: [5, 14, 27, 38, 41, 46, 92, 111, 135, 146, 154, 181]
失败的页面: [7, 40, 47, 200, 209, 220, 223]

文件位置:
  - CSV: compensate_retry_20241201_143022.csv
  - JSON: compensate_retry_20241201_143022.json
```

## 故障排除

### 1. 如果所有页面都失败

**可能原因：**
- 网站结构发生变化
- 网络连接问题
- 反爬虫机制

**解决方案：**
- 检查网络连接
- 尝试降低请求频率
- 手动访问几个页面URL确认

### 2. 如果部分页面成功

**说明：**
- 成功的页面确实有文章
- 失败的页面可能确实为空
- 这是正常情况

### 3. 如果脚本运行出错

**检查：**
- Chrome浏览器是否安装
- 网络连接是否正常
- 依赖包是否完整安装

## 数据合并

如果补偿脚本成功获取了新数据，可以使用合并功能：

```bash
python compensate_missing_pages.py merge
```

合并功能会：
1. 自动找到现有的数据文件
2. 自动找到补偿数据文件
3. 去重合并数据
4. 生成新的合并文件

## 注意事项

1. **网络友好**：补偿脚本保留了合理的延迟，不会给服务器造成压力
2. **数据安全**：所有数据都会实时保存，不会丢失
3. **资源管理**：自动管理浏览器资源，确保正确释放
4. **异常处理**：完善的错误处理机制

## 文件说明

| 文件名 | 用途 | 说明 |
|--------|------|------|
| `compensate_missing_pages.py` | 完整补偿脚本 | 功能全面，支持多种模式 |
| `quick_compensate.py` | 快速补偿脚本 | 简单快速，适合测试 |
| `compensate_missing.csv` | 补偿数据 | CSV格式的补偿结果 |
| `compensate_missing.json` | 补偿数据 | JSON格式的补偿结果 |

## 下一步

补偿完成后：

1. **检查结果**：查看补偿获取的文章数量
2. **数据合并**：将补偿数据合并到主数据文件
3. **质量检查**：检查数据的完整性和准确性
4. **最终统计**：更新总体的数据统计信息

---

**提示**：建议先运行 `quick_compensate.py` 进行快速测试，确认功能正常后再使用完整的补偿脚本。
